From c19b91211e6d794b9f9adcc1968971a1b48d9f6d Mon Sep 17 00:00:00 2001
From: Zhang Heng <zhangheng@mprc.pku.edu.cn>
Date: Sun, 11 Mar 2018 14:58:31 +0800
Subject: [PATCH 15/15] kvm: add vm exit handle framework

Signed-off-by: Zhang Heng <zhangheng@mprc.pku.edu.cn>
---
 arch/riscv/include/asm/csr.h      |   8 ++-
 arch/riscv/include/asm/kvm_host.h |   3 +
 arch/riscv/kvm/kvm_riscv.c        | 127 ++++++++++++++++++++++++++++++++++----
 3 files changed, 126 insertions(+), 12 deletions(-)

diff --git a/arch/riscv/include/asm/csr.h b/arch/riscv/include/asm/csr.h
index 0d64bc9..9b192d0 100644
--- a/arch/riscv/include/asm/csr.h
+++ b/arch/riscv/include/asm/csr.h
@@ -60,11 +60,17 @@
 #define EXC_BREAKPOINT          3
 #define EXC_LOAD_ACCESS         5
 #define EXC_STORE_ACCESS        7
-#define EXC_SYSCALL             8
+#define EXC_U_ECALL             8
+#define EXC_S_ECALL             9
+#define EXC_H_ECALL             10
+#define EXC_M_ECALL             11
 #define EXC_INST_PAGE_FAULT     12
 #define EXC_LOAD_PAGE_FAULT     13
+#define EXC_HOST_PAGE_FAULT     14
 #define EXC_STORE_PAGE_FAULT    15
 
+#define EXC_SYSCALL             EXC_U_ECALL
+
 #ifndef __ASSEMBLY__
 
 #define csr_swap(csr, val)					\
diff --git a/arch/riscv/include/asm/kvm_host.h b/arch/riscv/include/asm/kvm_host.h
index 75925ce..49c635a 100644
--- a/arch/riscv/include/asm/kvm_host.h
+++ b/arch/riscv/include/asm/kvm_host.h
@@ -180,4 +180,7 @@ void kvm_arch_vcpu_blocking(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_unblocking(struct kvm_vcpu *vcpu);
 void kvm_arch_vcpu_block_finish(struct kvm_vcpu *vcpu);
 
+int handle_vm_exit(struct kvm_vcpu *vcpu);
+int handle_h_ecall(struct kvm_vcpu *vcpu);
+int handle_host_page_fault(struct kvm_vcpu *vcpu);
 #endif /* __RISCV_KVM_HOST_H__ */
diff --git a/arch/riscv/kvm/kvm_riscv.c b/arch/riscv/kvm/kvm_riscv.c
index 8e1c57b..060617d 100644
--- a/arch/riscv/kvm/kvm_riscv.c
+++ b/arch/riscv/kvm/kvm_riscv.c
@@ -58,8 +58,7 @@ bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)
 
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 {
-	if (vcpu->mode == IN_GUEST_MODE)
-	{
+	if (vcpu->mode == IN_GUEST_MODE) {
 		vcpu->mode = EXITING_GUEST_MODE;
 		return 1;
 	}
@@ -233,28 +232,133 @@ extern asmlinkage void enter_virtual_machine(void *);
 
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
-	for(;;)
-	{
+	int ret;
+
+	for (;;) {
 		/*
 		 * setup sstatus, prepare going into vm.
 		 * leave sstatus.spie as before.
 		 */
-		if(vcpu->arch.prv)
-		{
+		if (vcpu->arch.prv) {
 			csr_set(sstatus, SR_PS);
-		}
-		else
-		{
+		} else {
 			csr_clear(sstatus, SR_PS);
 		}
 
 		enter_virtual_machine(&vcpu->arch);
-		printk("first exit\n");
-		return 0;
+
+		ret = handle_vm_exit(vcpu);
+		if (ret < 0) {
+			return ret;
+		}
+	}
+	return 0;
+}
+
+int handle_vm_exit(struct kvm_vcpu *vcpu)
+{
+	unsigned long scause;
+	int ret;
+
+	scause = csr_read_clear(scause, ~0L);
+	switch (scause) {
+		case EXC_H_ECALL:
+			ret = handle_h_ecall(vcpu);
+			break;
+		case EXC_HOST_PAGE_FAULT:
+			ret = handle_host_page_fault(vcpu);
+			break;
+		default:
+			return -ENOIOCTLCMD;
 	}
+
+	return ret;
+}
+
+int handle_h_ecall(struct kvm_vcpu *vcpu)
+{
+	unsigned long h_ecall_type = vcpu->arch.gpr[17];
+	unsigned long arg0 = vcpu->arch.gpr[10];
+	unsigned long arg1 = vcpu->arch.gpr[11];
+	unsigned long retval = 0;
+
+	switch (h_ecall_type) {
+		case SBI_CONSOLE_PUTCHAR:
+		case SBI_CONSOLE_GETCHAR:
+		case SBI_SHUTDOWN:
+			return -1;
+		case SBI_SET_TIMER:
+		case SBI_SEND_IPI:
+		case SBI_REMOTE_SFENCE_VMA:
+		case SBI_REMOTE_SFENCE_VMA_ASID:
+		case SBI_REMOTE_FENCE_I:
+		case SBI_CLEAR_IPI:
+		default:
+			return -ENOIOCTLCMD;
+	}
+	vcpu->arch.gpr[10] = retval;
+
 	return 0;
 }
 
+static int build_tdp_page_table(struct kvm_vcpu *vcpu, gfn_t gfn, kvm_pfn_t pfn)
+{
+	struct kvm* kvm = vcpu->kvm;
+	int levels = 3;
+	int pte_size = 8;
+	int pt_idx_bits = 9;
+	int pt_shift = (levels - 1) * pt_idx_bits;
+	int idx;
+	unsigned long base, pte_addr, pte, tmp_addr;
+
+	base = (unsigned long)kvm->arch.gpa_mm.pgd;
+	for (;pt_shift >= 0;pt_shift -= pt_idx_bits) {
+		idx = (gfn >> pt_shift) & ((1 << pt_idx_bits) - 1);
+		pte_addr = base + idx * pte_size;
+		pte = *((unsigned long *)pte_addr);
+		if (!(pte & _PAGE_PRESENT)) {
+			if (pt_shift > 0) {
+				tmp_addr = __get_free_page(GFP_KERNEL
+						| __GFP_RETRY_MAYFAIL | __GFP_ZERO);
+				if (tmp_addr == 0) {
+					return -1;
+				}
+				pte = (virt_to_pfn(tmp_addr) << _PAGE_PFN_SHIFT)
+					| _PAGE_PRESENT;
+			} else {
+				pte = (pfn << _PAGE_PFN_SHIFT)
+					| _PAGE_PRESENT | _PAGE_READ | _PAGE_WRITE | _PAGE_EXEC
+					| _PAGE_USER;
+			}
+			*((unsigned long *)pte_addr) = pte;
+		} else {
+			tmp_addr = pfn_to_virt(pte >> _PAGE_PFN_SHIFT);
+		}
+		base = tmp_addr;
+	}
+
+	return 0;
+}
+
+int handle_host_page_fault(struct kvm_vcpu *vcpu)
+{
+	struct kvm_memory_slot *slot;
+	unsigned long gpa = csr_read(hvtval);
+	gfn_t gfn;
+	kvm_pfn_t pfn;
+	bool async, writable;
+
+	gfn = gpa >> PAGE_SHIFT;
+	slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
+	if (slot == NULL)
+		return -1; /* Maybe mmio, not handle now */
+
+	pfn = __gfn_to_pfn_memslot(slot, gfn, false, &async, false, &writable);
+	/* What to do if async is true? */
+
+	return build_tdp_page_table(vcpu, gfn, pfn);
+}
+
 int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
 					struct kvm_mp_state *mp_state)
 {
@@ -466,6 +570,7 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	vcpu->arch.scause = csr_read_clear(bscause, ~0L);
 	vcpu->arch.stval = csr_read_clear(bstval, ~0L);
 	vcpu->arch.satp = csr_read_clear(bsatp, ~0L);
+	csr_clear(sgatp, ~0L);
 
 	local_irq_restore(flags);
 }
-- 
2.7.4

